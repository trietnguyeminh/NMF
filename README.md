# NMF
first prj
TiÃªu Ä‘á»: PhÃ¢n tÃ­ch Ma tráº­n KhÃ´ng Ã¢m & á»¨ng dá»¥ng 1Slide 1, 2, 3 2
Xin chÃ o Tháº§y (CÃ´) vÃ  cÃ¡c báº¡n. ChÃºng em lÃ  NhÃ³m 4, lá»›p CS115.Q113. Trong buá»•i hÃ´m nay, nhÃ³m chÃºng em xin trÃ¬nh bÃ y vá» má»™t ká»¹ thuáº­t phÃ¢n rÃ£ ma tráº­n chuyÃªn sÃ¢u: Non-Negative Matrix Factorization (NMF) hay cÃ²n gá»i lÃ  PhÃ¢n tÃ­ch Ma tráº­n KhÃ´ng Ã¢m, cÃ¹ng vá»›i cÆ¡ sá»Ÿ toÃ¡n há»c vÃ  káº¿t quáº£ thá»±c nghiá»‡m cá»§a nÃ³4.
(Giá»›i thiá»‡u thÃ nh viÃªn) 5555
Ná»™i dung trÃ¬nh bÃ y cá»§a nhÃ³m sáº½ bao gá»“m 4 pháº§n chÃ­nh6:
Váº¥n Ä‘á» cá»§a PCA vÃ  Giáº£i phÃ¡p NMF
CÃ¡c dá»¯ liá»‡u thá»±c táº¿ nhÆ° hÃ¬nh áº£nh (pixel) hay vÄƒn báº£n (táº§n suáº¥t tá»«) Ä‘á»u Ä‘Æ°á»£c biá»ƒu diá»…n dÆ°á»›i dáº¡ng ma tráº­n khÃ´ng Ã¢m11. Äá»ƒ giáº£m chiá»u dá»¯ liá»‡u nÃ y, cÃ¡ch tiáº¿p cáº­n kinh Ä‘iá»ƒn lÃ  sá»­ dá»¥ng SVD hoáº·c PCA12.
Tuy nhiÃªn, cÃ¡c phÆ°Æ¡ng phÃ¡p nÃ y tá»“n táº¡i má»™t háº¡n cháº¿ lá»›n vá» kháº£ nÄƒng diá»…n giáº£i (Interpretability)13. PCA cho phÃ©p cÃ¡c vector cÆ¡ sá»Ÿ vÃ  há»‡ sá»‘ mang giÃ¡ trá»‹ Ã¢m14. Äiá»u nÃ y dáº«n Ä‘áº¿n viá»‡c tÃ¡i táº¡o dá»¯ liá»‡u thÃ´ng qua cÃ¡c phÃ©p cá»™ng trá»« phá»©c táº¡p, khiáº¿n cÃ¡c thÃ nh pháº§n triá»‡t tiÃªu láº«n nhau (cancellation effect)15.
Vá» máº·t toÃ¡n há»c, sai sá»‘ cÃ³ thá»ƒ nhá», nhÆ°ng cÃ¡c Ä‘áº·c trÆ°ng trÃ­ch xuáº¥t Ä‘Æ°á»£c (nhÆ° Eigenfaces) thÆ°á»ng mang tÃ­nh toÃ n thá»ƒ (holistic) vÃ  ráº¥t khÃ³ Ä‘á»ƒ con ngÆ°á»i hiá»ƒu Ä‘Æ°á»£c Ã½ nghÄ©a váº­t lÃ½ cá»§a chÃºng16.
Xuáº¥t phÃ¡t tá»« háº¡n cháº¿ Ä‘Ã³, NMF ra Ä‘á»i17. PhÆ°Æ¡ng phÃ¡p nÃ y Ã¡p Ä‘áº·t rÃ ng buá»™c khÃ´ng Ã¢m, buá»™c thuáº­t toÃ¡n pháº£i há»c cÃ¡c Ä‘áº·c trÆ°ng cá»¥c bá»™ (parts-based), giÃºp dá»¯ liá»‡u Ä‘Æ°á»£c biá»ƒu diá»…n má»™t cÃ¡ch tá»± nhiÃªn hÆ¡n thÃ´ng qua cÃ¡c phÃ©p cá»™ng gá»™p thuáº§n tÃºy18.
Slide 4, 5, 6: Váº¥n Ä‘á» cá»§a PCA vs. Giáº£i phÃ¡p NMF 19
Äá»ƒ hiá»ƒu rÃµ Ä‘á»™ng lá»±c ra Ä‘á»i cá»§a NMF, chÃºng ta cáº§n nhÃ¬n láº¡i báº£n cháº¥t toÃ¡n há»c cá»§a viá»‡c phÃ¢n rÃ£ ma tráº­n. Cáº£ PCA vÃ  NMF Ä‘á»u biá»ƒu diá»…n dá»¯ liá»‡u gá»‘c V dÆ°á»›i dáº¡ng má»™t tá»• há»£p tuyáº¿n tÃ­nh (Linear Combination) cá»§a cÃ¡c vector cÆ¡ sá»Ÿ. Tuy nhiÃªn, sá»± khÃ¡c biá»‡t náº±m á»Ÿ cÃ¡c rÃ ng buá»™c:
â€¢	Trong PCA (hay SVD), cÃ¡c vector cÆ¡ sá»Ÿ (Eigenvectors) pháº£i thá»a mÃ£n tÃ­nh trá»±c giao vÃ  hÆ°á»›ng tá»›i phÆ°Æ¡ng sai lá»›n nháº¥t. Quan trá»ng hÆ¡n, cÃ¡c há»‡ sá»‘ nÃ y cÃ³ thá»ƒ mang giÃ¡ trá»‹ Ã¢m. Äiá»u nÃ y dáº«n Ä‘áº¿n má»™t hiá»‡n tÆ°á»£ng gá»i lÃ  sá»± triá»‡t tiÃªu (complex cancellation). Äá»ƒ tÃ¡i táº¡o má»™t bá»©c áº£nh khuÃ´n máº·t, PCA cÃ³ thá»ƒ láº¥y má»™t khuÃ´n máº·t máº«u, rá»“i cá»™ng thÃªm má»™t thÃ nh pháº§n, sau Ä‘Ã³ trá»« Ä‘i má»™t thÃ nh pháº§n khÃ¡c Ä‘á»ƒ xÃ³a bá» cÃ¡c chi tiáº¿t thá»«a. ChÃ­nh vÃ¬ cÆ¡ cháº¿ cá»™ng-trá»« phá»©c táº¡p nÃ y, cÃ¡c vector cÆ¡ sá»Ÿ cá»§a PCA (nhÆ° hÃ¬nh bÃªn trÃ¡i) mang tÃ­nh toÃ n thá»ƒ (holistic)â€“ chÃºng trÃ´ng giá»‘ng nhÆ° nhá»¯ng khuÃ´n máº·t mÃ©o mÃ³, chá»“ng cháº­p lÃªn nhau, khÃ´ng mang má»™t Ã½ nghÄ©a váº­t lÃ½ cá»¥ thá»ƒ nÃ o mÃ  con ngÆ°á»i cÃ³ thá»ƒ gá»i tÃªn.
â€¢	NMF giáº£i quyáº¿t váº¥n Ä‘á» nÃ y báº±ng má»™t rÃ ng buá»™c tá»± nhiÃªn: táº¥t cáº£ ma tráº­n V, W, vÃ  H Ä‘á»u pháº£i lá»›n hÆ¡n hoáº·c báº±ng 024. Khi khÃ´ng cho phÃ©p phÃ©p trá»«, chÃºng ta chá»‰ cÃ³ thá»ƒ tÃ¡i táº¡o dá»¯ liá»‡u báº±ng cÃ¡ch cá»™ng gá»™p (additive parts). HÃ£y tÆ°á»Ÿng tÆ°á»£ng: Náº¿u báº¡n khÃ´ng thá»ƒ â€™trá»«â€™ Ä‘i cÃ¡i mÅ©i tá»« má»™t khuÃ´n máº·t, thÃ¬ cÃ¡ch duy nháº¥t Ä‘á»ƒ biá»ƒu diá»…n khuÃ´n máº·t Ä‘Ã³ lÃ  báº¡n pháº£i cÃ³ má»™t vector cÆ¡ sá»Ÿ chá»©a sáºµn cÃ¡i mÅ©i, má»™t vector khÃ¡c chá»©a sáºµn con máº¯t, vÃ  ghÃ©p chÃºng láº¡i. ÄÃ¢y chÃ­nh lÃ  lÃ½ do táº¡i sao NMF (hÃ¬nh bÃªn pháº£i) cÃ³ kháº£ nÄƒng há»c Ä‘Æ°á»£c cÃ¡c biá»ƒu diá»…n dá»±a trÃªn bá»™ pháº­n (parts-based representation). CÃ¡c vector cÆ¡ sá»Ÿ há»c Ä‘Æ°á»£c trá»Ÿ nÃªn thÆ°a (sparse) vÃ  khu trÃº táº¡i cÃ¡c vÃ¹ng Ä‘áº·c trÆ°ng (nhÆ° máº¯t, mÅ©i, miá»‡ng). Äiá»u nÃ y khÃ´ng chá»‰ Ä‘Ãºng vá» máº·t toÃ¡n há»c mÃ  cÃ²n tÆ°Æ¡ng Ä‘á»“ng vá»›i cÆ¡ cháº¿ nháº­n thá»©c cá»§a nÃ£o bá»™ con ngÆ°á»i.
________________________________________
PHáº¦N 2: CÆ  Sá» TOÃN Há»ŒC & PHÃ‚N TÃCH HÃ€M Máº¤T MÃT (6 PhÃºt)
Slide 7, 8, 9: BÃ i toÃ¡n tá»‘i Æ°u hÃ³a 28
Vá» cÆ¡ báº£n, NMF lÃ  bÃ i toÃ¡n tÃ¬m kiáº¿m sá»± xáº¥p xá»‰ cho ma tráº­n dá»¯ liá»‡u $V$ (kÃ­ch thÆ°á»›c $m \times n$) báº±ng tÃ­ch cá»§a hai ma tráº­n háº¡ng tháº¥p (low-rank):
â€¢	Ma tráº­n W (mk) Chá»©a cÃ¡c cÆ¡ sá»Ÿ Ä‘áº·c trÆ°ng (basis vectors)29.
â€¢	Ma tráº­n H (kn): Chá»©a cÃ¡c há»‡ sá»‘ kÃ­ch hoáº¡t (coefficients) tÆ°Æ¡ng á»©ng30.
$k$ lÃ  sá»‘ chiá»u áº©n, thÆ°á»ng nhá» hÆ¡n ráº¥t nhiá»u so vá»›i m vÃ  n, giÃºp nÃ©n dá»¯ liá»‡u hiá»‡u quáº£
ThÃ¡ch thá»©c cá»‘t lÃµi khi giáº£i bÃ i toÃ¡n PhÃ¢n tÃ­ch ma tráº­n khÃ´ng Ã¢m (NMF) náº±m á»Ÿ HÃ m Má»¥c TiÃªu (Loss Function) D(V||WH). Má»¥c tiÃªu lÃ  tÃ¬m W â‰¥ 0 vÃ  H â‰¥ 0 Ä‘á»ƒ tá»‘i thiá»ƒu hÃ³a Loss funct
ThÃ¡ch Thá»©c ToÃ¡n Há»c vÃ  Giáº£i PhÃ¡p
â€¢	HÃ m má»¥c tiÃªu cá»§a bÃ i toÃ¡n nÃ y lÃ  khÃ´ng lá»“i (non-convex) trÃªn toÃ n cá»¥c (tá»©c lÃ  khi xÃ©t Ä‘á»“ng thá»i cáº£ hai ma tráº­n W vÃ  H). 
â€¢	TÃ­nh cháº¥t: Má»™t hÃ m lá»“i giá»‘ng nhÆ° má»™t cÃ¡i bÃ¡t, chá»‰ cÃ³ má»™t Cá»±c Tiá»ƒu ToÃ n Cá»¥c (Global Minimum). NgÆ°á»£c láº¡i, tÃ­nh khÃ´ng lá»“i táº¡o ra má»™t bá» máº·t phá»©c táº¡p vá»›i nhiá»u Cá»±c Tiá»ƒu Äá»‹a PhÆ°Æ¡ng (Local Minima). 
â€¢	Háº­u quáº£: ChÃ­nh vÃ¬ tÃ­nh khÃ´ng lá»“i nÃ y, chÃºng ta khÃ´ng cÃ³ lá»i giáº£i Ä‘Ã³ng (closed-form solution). Thuáº­t toÃ¡n tá»‘i Æ°u hÃ³a cÃ³ thá»ƒ bá»‹ káº¹t táº¡i má»™t cá»±c tiá»ƒu Ä‘á»‹a phÆ°Æ¡ng báº¥t ká»³, khiáº¿n cho cháº¥t lÆ°á»£ng lá»i giáº£i khÃ´ng Ä‘Æ°á»£c Ä‘áº£m báº£o.
â€¢	Giáº£i PhÃ¡p: Khai ThÃ¡c TÃ­nh Bi-Convexity: Äá»ƒ kháº¯c phá»¥c, chÃºng ta khai thÃ¡c tÃ­nh cháº¥t Lá»“i RiÃªng Láº» (Bi-convexity)36. HÃ m má»¥c tiÃªu lÃ  lá»“i trÃªn $W$ khi ta cá»‘ Ä‘á»‹nh $H$, vÃ  ngÆ°á»£c láº¡i, nÃ³ lÃ  lá»“i trÃªn $H$ khi ta cá»‘ Ä‘á»‹nh $W$37. TÃ­nh cháº¥t nÃ y cho phÃ©p chÃºng ta xÃ¢y dá»±ng cÃ¡c Giáº£i Thuáº­t Láº·p (Iterative Updates) báº±ng cÃ¡ch tá»‘i Æ°u hÃ³a luÃ¢n phiÃªn $W$ vÃ  $H$38. hÃ m má»¥c tiÃªu sáº½ luÃ´n giáº£m trong má»—i bÆ°á»›c vÃ  há»™i tá»¥ vá» má»™t cá»±c tiá»ƒu Ä‘á»‹a phÆ°Æ¡ng	
ChÃ¬a KhÃ³a Cháº¥t LÆ°á»£ng: Lá»±a chá»n Äá»™ Ä‘o KhÃ¡c biá»‡t (Divergence)
Cháº¥t lÆ°á»£ng cá»§a káº¿t quáº£ NMF Ä‘Æ°á»£c quyáº¿t Ä‘á»‹nh bá»Ÿi viá»‡c lá»±a chá»n Ä‘á»™ Ä‘o sá»± khÃ¡c biá»‡t HÃ m Máº¥t MÃ¡t39. Má»—i hÃ m $D$ tÆ°Æ¡ng á»©ng vá»›i má»™t giáº£ Ä‘á»‹nh thá»‘ng kÃª khÃ¡c nhau vá» dá»¯ liá»‡u Ä‘áº§u vÃ o vÃ  nhiá»…u40404040.
Slide 10: PhÃ¢n tÃ­ch ToÃ¡n há»c 3 HÃ m Divergence 41
1. ğŸ“ Chuáº©n Frobenius (Frobenius Norm) - $L_2$
â€¢	Báº£n cháº¥t ToÃ¡n há»c: ÄÃ¢y lÃ  Ä‘á»™ Ä‘o sai sá»‘ kinh Ä‘iá»ƒn, thá»±c cháº¥t lÃ  tá»•ng bÃ¬nh phÆ°Æ¡ng sai sá»‘ (Least Squares)â€”tá»©c lÃ  khoáº£ng cÃ¡ch Euclid giá»¯a hai ma tráº­n10.
â€¢	CÆ¡ sá»Ÿ Thá»‘ng kÃª (MLE): Viá»‡c tá»‘i Æ°u hÃ³a hÃ m nÃ y tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i bÃ i toÃ¡n Æ¯á»›c lÆ°á»£ng Há»£p lÃ½ Cá»±c Ä‘áº¡i (MLE) khi giáº£ Ä‘á»‹nh nhiá»…u cá»™ng (Additive Noise) tuÃ¢n theo PhÃ¢n phá»‘i Chuáº©n (Gaussian)11.
o	Giáº£i thÃ­ch: Nhiá»…u Cá»™ng ($\epsilon$) Ä‘Æ°á»£c cá»™ng tháº³ng vÃ o tÃ­n hiá»‡u gá»‘c ($V \approx WH + \epsilon$). PhÃ¢n phá»‘i Chuáº©n lÃ  phÃ¢n phá»‘i Ä‘á»‘i xá»©ng, táº­p trung sai sá»‘ quanh 0.
â€¢	Háº­u quáº£ Thao tÃ¡c: VÃ¬ cÃ´ng thá»©c chá»©a phÃ©p bÃ¬nh phÆ°Æ¡ng $(...)^2$ 12, nÃ³ trá»«ng pháº¡t cá»±c náº·ng cÃ¡c sai sá»‘ lá»›n (outliers)13. Äá»ƒ giáº£m thiá»ƒu tá»•ng sai sá»‘, thuáº­t toÃ¡n cÃ³ xu hÆ°á»›ng 'dÃ n Ä‘á»u sai sá»‘' ra cÃ¡c Ä‘iá»ƒm lÃ¢n cáº­n14.
â€¢	ÄÃ¡nh giÃ¡: Äiá»u nÃ y dáº«n Ä‘áº¿n hiá»‡u á»©ng lÃ m mÆ°á»£t (smoothing), khiáº¿n káº¿t quáº£ tÃ¡i táº¡o thÆ°á»ng bá»‹ nhÃ²e vÃ  máº¥t Ä‘i Ä‘á»™ sáº¯c nÃ©t á»Ÿ cÃ¡c cáº¡nh chi tiáº¿t15.
2. ğŸ“š Kullback-Leibler (KL) Divergence
â€¢	Báº£n cháº¥t ToÃ¡n há»c: ÄÃ¢y lÃ  Ä‘á»™ Ä‘o Ä‘á»™ lá»‡ch thÃ´ng tin (Relative Entropy)16. KhÃ¡c vá»›i Frobenius, hÃ m nÃ y lÃ  báº¥t Ä‘á»‘i xá»©ng17.
HÃ¬nh pháº¡t:
â€¢	Pháº¡t cá»±c náº·ng (tien vo cung) khi $WH$ Ä‘Ã¡nh giÃ¡ quÃ¡ tháº¥p (khi $WH \to 0$).
â€¢	HÃ¬nh pháº¡t cho viá»‡c Ä‘Ã¡nh giÃ¡ quÃ¡ cao ($WH > V$) cÅ©ng tÄƒng nhÆ°ng má»©c Ä‘á»™ nháº¹ hÆ¡n nhiá»u so vá»›i bÃªn trÃ¡i.
o		
â€¢	CÆ¡ sá»Ÿ Thá»‘ng kÃª (MLE): HÃ m KL tÆ°Æ¡ng á»©ng vá»›i giáº£ Ä‘á»‹nh dá»¯ liá»‡u tuÃ¢n theo PhÃ¢n phá»‘i Poisson18.
o	Giáº£i thÃ­ch: PhÃ¢n phá»‘i Poisson chuyÃªn dÃ¹ng Ä‘á»ƒ mÃ´ táº£ cÃ¡c sá»± kiá»‡n Ä‘áº¿m Ä‘Æ°á»£c (count events), nhÆ° táº§n suáº¥t tá»« xuáº¥t hiá»‡n trong vÄƒn báº£n19.
â€¢	Háº­u quáº£ Thao tÃ¡c: HÃ m KL khuyáº¿n khÃ­ch tÃ­nh thÆ°a (sparsity)20. Vá» máº·t váº­t lÃ½, nÃ³ giÃºp thuáº­t toÃ¡n tÃ¡ch biá»‡t rÃµ rÃ ng pháº§n 'ná»n' (giÃ¡ trá»‹ 0) vÃ  pháº§n 'tÃ­n hiá»‡u' (nÃ©t chá»¯).
â€¢	ÄÃ¡nh giÃ¡: ÄÃ¢y lÃ  lá»±a chá»n tá»‘i Æ°u cho dá»¯ liá»‡u thÆ°a (sparse) nhÆ° áº£nh chá»¯ viáº¿t tay hoáº·c vÄƒn báº£n 21, táº¡o ra cÃ¡c Ä‘áº·c trÆ°ng parts-based sáº¯c nÃ©t hÆ¡n so vá»›i Frobenius22.
3. ğŸ“¢ Itakura-Saito (IS) Divergence
â€¢	Báº£n cháº¥t ToÃ¡n há»c: HÃ m nÃ y Ä‘o sá»± khÃ¡c biá»‡t dá»±a trÃªn tá»· lá»‡ (ratio) thay vÃ¬ hiá»‡u sá»‘23.
â€¢	CÆ¡ sá»Ÿ Thá»‘ng kÃª (MLE): HÃ m IS tÆ°Æ¡ng á»©ng vá»›i giáº£ Ä‘á»‹nh nhiá»…u nhÃ¢n (Multiplicative Noise) tuÃ¢n theo PhÃ¢n phá»‘i Gamma24.
o	Giáº£i thÃ­ch: Nhiá»…u NhÃ¢n lÃ  khi cÆ°á»ng Ä‘á»™ nhiá»…u tá»· lá»‡ thuáº­n vá»›i cÆ°á»ng Ä‘á»™ tÃ­n hiá»‡u gá»‘c.
â€¢	Háº­u quáº£ Thao tÃ¡c: Äáº·c tÃ­nh quan trá»ng nháº¥t lÃ  Báº¥t biáº¿n theo tá»· lá»‡ (Scale Invariance)25. VÃ­ dá»¥, sai sá»‘ giá»¯a (1 vÃ  10) Ä‘Æ°á»£c coi trá»ng ngang báº±ng sai sá»‘ giá»¯a (10 vÃ  100) vÃ¬ tá»· lá»‡ cá»§a chÃºng Ä‘á»u lÃ  1026.
â€¢	ÄÃ¡nh giÃ¡:
o	Æ¯u Ä‘iá»ƒm: Ráº¥t máº¡nh trong xá»­ lÃ½ Ã¢m thanh (do tai ngÆ°á»i nghe theo thang logarit)27.
o	NhÆ°á»£c Ä‘iá»ƒm chÃ­ máº¡ng: CÃ´ng thá»©c chá»©a phÃ©p chia cho ma tráº­n tÃ¡i táº¡o $WH$28. Vá»›i dá»¯ liá»‡u áº£nh cÃ³ nhiá»u Ä‘iá»ƒm Ä‘en (pixel $\approx 0$), máº«u sá»‘ tiáº¿n vá» 0 khiáº¿n biá»ƒu thá»©c tiáº¿n tá»›i vÃ´ cÃ¹ng29. Äiá»u nÃ y gÃ¢y ra sá»± máº¥t á»•n Ä‘á»‹nh sá»‘ há»c (Numerical Instability) vÃ  lá»—i chia cho 030.

PHáº¦N 3: CHá»¨NG MINH & TRIá»‚N KHAI THUáº¬T TOÃN (5 PhÃºt)
Slide 11, 12, 13: Tá»« Gradient Descent Ä‘áº¿n Multiplicative Update Rules (MUR) 58
Sau khi chá»n Ä‘Æ°á»£c hÃ m máº¥t mÃ¡t, chÃºng ta Ä‘á»‘i máº·t vá»›i bÃ i toÃ¡n tá»‘i Æ°u hÃ³a cÃ³ rÃ ng buá»™c: TÃ¬m $W, H$ sao cho hÃ m lá»—i $J$ nhá» nháº¥t, nhÆ°ng vá»›i Ä‘iá»u kiá»‡n $W, H \ge 0$59.
ThÃ´ng thÆ°á»ng, chÃºng ta nghÄ© ngay Ä‘áº¿n Gradient Descent60. Tuy nhiÃªn, phÃ©p trá»« vector gradient trong GD cÃ³ thá»ƒ Ä‘áº©y giÃ¡ trá»‹ cá»§a $H$ xuá»‘ng vÃ¹ng Ã¢m61. Náº¿u dÃ¹ng GD thuáº§n tÃºy, chÃºng ta pháº£i liÃªn tá»¥c thá»±c hiá»‡n phÃ©p chiáº¿u (Projected Gradient Descent) Ä‘á»ƒ gÃ¡n cÃ¡c giÃ¡ trá»‹ Ã¢m vá» 0, lÃ m thuáº­t toÃ¡n cháº­m vÃ  kÃ©m hiá»‡u quáº£62.
Äá»ƒ giáº£i quyáº¿t, tÃ´i sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p Cáº­p nháº­t nhÃ¢n (Multiplicative Update Rules - MUR) cá»§a Lee & Seung63. ÄÃ¢y lÃ  cÃ¡ch chá»©ng minh cÃ´ng thá»©c nÃ y dá»±a trÃªn quan Ä‘iá»ƒm vá» Learning Rate thÃ­ch á»©ng:

        
Káº¿t luáº­n: ChÃºng ta Ä‘Ã£ biáº¿n phÃ©p trá»« thÃ nh phÃ©p nhÃ¢n75. Náº¿u khá»Ÿi táº¡o $W, H$ dÆ°Æ¡ng, quy táº¯c cáº­p nháº­t nÃ y Ä‘áº£m báº£o káº¿t quáº£ luÃ´n dÆ°Æ¡ng mÃ£i mÃ£i mÃ  khÃ´ng cáº§n kiá»ƒm tra Ä‘iá»u kiá»‡n biÃªn76. Äá»“ng thá»i, khi Ä‘áº¡o hÃ m báº±ng 0 $(\nabla^{+}=\nabla^{-})$, tá»‰ sá»‘ nÃ y báº±ng 1, $H$ sáº½ khÃ´ng Ä‘á»•i, nghÄ©a lÃ  thuáº­t toÃ¡n Ä‘Ã£ há»™i tá»¥ táº¡i Ä‘iá»ƒm cá»±c trá»‹77.
ThÃ¡ch thá»©c Ká»¹ thuáº­t vÃ  giáº£i phÃ¡p 78
Viá»‡c chá»©ng minh toÃ¡n há»c ráº¥t Ä‘áº¹p, nhÆ°ng khi triá»ƒn khai thá»±c táº¿ (Implementation) trÃªn mÃ¡y tÃ­nh, tÃ´i gáº·p pháº£i váº¥n Ä‘á» vá» TÃ­nh á»•n Ä‘á»‹nh sá»‘ há»c (Numerical Stability)79.
â€¢	ThÃ¡ch thá»©c thá»±c táº¿: Dá»¯ liá»‡u áº£nh ráº¥t thÆ°a (sparse)80. Khi thuáº­t toÃ¡n hoáº¡t Ä‘á»™ng, ma tráº­n tÃ¡i táº¡o $WH$ cÃ³ thá»ƒ tiáº¿n dáº§n vá» 081.
o	Vá»›i IS Divergence, cÃ´ng thá»©c cáº­p nháº­t cÃ³ phÃ©p chia cho $(WH)^{2}$. Khi máº«u sá»‘ nÃ y tiáº¿n vá» 0, mÃ¡y tÃ­nh sáº½ gáº·p lá»—i ZeroDivisionError82.
o	Vá»›i KL Divergence, cÃ´ng thá»©c chá»©a phÃ©p toÃ¡n $log(WH)$. Khi $WH$ tiáº¿n vá» 0, $log(WH)$ sáº½ lÃ  Ã¢m vÃ´ cÃ¹ng, khiáº¿n mÃ¡y tÃ­nh tráº£ vá» NaN (Not a Number), lÃ m thuáº­t toÃ¡n sá»¥p Ä‘á»•83.
â€¢	Giáº£i phÃ¡p Ká»¹ thuáº­t - Smoothing: Äá»ƒ kháº¯c phá»¥c, tÃ´i Ã¡p dá»¥ng ká»¹ thuáº­t lÃ m trÆ¡n (Smoothing), má»™t chuáº©n má»±c trong tÃ­nh toÃ¡n sá»‘84. TÃ´i cá»™ng thÃªm má»™t háº±ng sá»‘ cá»±c nhá» $\epsilon$ (Machine Epsilon, thÆ°á»ng lÃ  $10^{-9}$ hoáº·c $10^{-16}$) vÃ o máº«u sá»‘ hoáº·c bÃªn trong hÃ m $log$85.
(Chá»‰ vÃ o code) 86
VÃ­ dá»¥ trong hÃ m update_kl: numerator W.T @ (V/ (WH+ epsilon)) (TrÃ¡nh chia cho 0) 87
Viá»‡c thÃªm $\epsilon$ nÃ y Ä‘Ã³ng vai trÃ² nhÆ° má»™t bá»™ Ä‘iá»u hÃ²a (regularizer) sá»‘ há»c, Ä‘áº£m báº£o cÃ¡c phÃ©p toÃ¡n luÃ´n xÃ¡c Ä‘á»‹nh mÃ  sai sá»‘ gÃ¢y ra lÃ  khÃ´ng Ä‘Ã¡ng ká»ƒ, giÃºp thuáº­t toÃ¡n há»™i tá»¥ á»•n Ä‘á»‹nh ngay cáº£ trÃªn cÃ¡c táº­p dá»¯ liá»‡u cá»±c thÆ°a88.

